{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ текстов\n",
    "Какие проблемы возникают?\n",
    "    1. Тексты грязные, с опечатками\n",
    "    2. Как формировать признаки?\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Приведение к нижнему регистру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lower(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. Удаление пунктуации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    exclude = r'[!?,.:;()\"*/\\\\_]'\n",
    "    s = re.sub(exclude, ' ', text)\n",
    "    s = re.sub('-', '', s)\n",
    "    return ' '.join(s.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3. Приведение к нормальной форме\n",
    "Врача -> Врач, делали -> делать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "MORPH = pymorphy2.MorphAnalyzer()\n",
    "NORMAL_FORM = {}\n",
    "\n",
    "# сначала научимся работать со словами\n",
    "def get_word_normal_form(word):\n",
    "    if word not in NORMAL_FORM:\n",
    "        NORMAL_FORM[word] = MORPH.parse(word)[0].normal_form\n",
    "    return NORMAL_FORM[word]\n",
    "\n",
    "# а затем с текстом\n",
    "def get_sentence_normal_form(sentence):\n",
    "    return ' '.join(get_word_normal_form(word) for word in sentence.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4. Что еще?..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Переход к признакам\n",
    "    1. Bag of words (CountVectorizer, HashingVectorizer)\n",
    "    2. Tf-idf Vectorizer\n",
    "    3. word2vec (doc2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bag of words\n",
    "![](http://www.python-course.eu/images/document_representation.png)\n",
    "CountVectorizer - просто считает вхождения разных слов. HashingVectorizer - то же самое, но размер выборки снижается за счет того, что некоторые значения кодируются одним и тем же хешом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat = ['talk.religion.misc','soc.religion.christian'] # Выбираем две категории для классификации\n",
    "# Данные для обучения\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories = cat, shuffle=True, random_state=1\n",
    "                                  ,remove=('headers', 'footers', 'quotes'))\n",
    "# Данне для проверки\n",
    "twenty_test = fetch_20newsgroups(subset='test', categories = cat, shuffle=True, random_state=1\n",
    "                                  ,remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer() # можно задавать ограничения\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tf-Idf (term frequency - inverse document frequency)\n",
    "Идея в том, чтобы оценивать важность слова в пределах документа и понижать важность для общеупотребительных слов\n",
    "![](http://www.joyofdata.de/blog/wp-content/uploads/2014/02/tf-idf.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Word2vec\n",
    "В кратце - с помощью нейросети обучить сжатое векторное представление слова на основе его сочетания(контекста) с другими словами\n",
    "\n",
    "Есть готовые натренированные корпусы текстов, но можно обучить и на своих\n",
    "\n",
    "http://nlpx.net/archives/179\n",
    "\n",
    "http://mccormickml.com/assets/word2vec/Alex_Minnaar_Word2Vec_Tutorial_Part_I_The_Skip-Gram_Model.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-d3a9c68dc0ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw2v_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ds' is not defined"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "w2v_model = word2vec.Word2Vec.load_word2vec_format('/Users/admin/Downloads/', binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# типичный пример"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом можно получить векторные представления слов. А что делать с предложениями?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding(sentence):\n",
    "    words = list(filter(lambda x: x in w2v_model, sentence.split()))\n",
    "    if len(words) == 0:\n",
    "        return None\n",
    "    vectors = list(map(lambda x: w2v_model[x], words))\n",
    "    embedding = np.mean(vectors, axis=0) #max,median,min...\n",
    "    embedding /= np.linalg.norm(embedding, axis=0)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полезно делать  **взвешенное** по Tf-Idf среднее!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кое-что еще\n",
    "    Хорошо работают рекуррентные/сверточные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo:\n",
    "    1. Реализовать взвешенное по tf-idf word2vec\n",
    "    2. Реализовать полный цикл препроцессинга слов\n",
    "    3. Сравнить качество классификации по следующим моделям на логистической регрессии (по train/test):\n",
    "        3.1. CountVectorizer\n",
    "        3.2. HashingVectorizer(несколько разных размеров слов)\n",
    "        3.3. Tf-Idf\n",
    "        3.4. Word2Vec натренированный по коллекции слов документа (среднее/макс/минимум)\n",
    "        3.5. Word2Vec корпус из интернета (среднее/макс/минимум)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# если сложно с такими размерностями, то можно попробовать PCA\n",
    "from sklearn.decomposition import PCA,SparsePCA,TruncatedSVD\n",
    "pca = TruncatedSVD(n_components=100)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var_rat = pca.explained_variance_ratio_\n",
    "#как мы видим какого-то колоссального отрыва у небольшого числа компонент нету\n",
    "np.sort(var_rat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
